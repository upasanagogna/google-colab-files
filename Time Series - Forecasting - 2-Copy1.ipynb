{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Time Series - Forecasting - 2-Copy1.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"c2mCTvm4J_RN"},"source":["# 1 Time Series Forecasting- Azure\n","\n","### Definition – Anomaly Detection\n","Making predictions about the future is called extrapolation in the classical statistical handling of time series data.\n","\n","Forecasting involves taking models fit on historical data and using them to predict future\n","observations.\n","\n","\n","\n","In this Notebook we would be discussing the various Microsoft Azure based Time Series - forecasting Solutions.\n","\n","\n","Note: For EDA (Exploratory Data Analysis), there are 2 options\n","- [Azure Data Explorer](#1.3.4)\n","- [Custom Code](#1.3.4)\n","\n","\n","For performing Time Series - Anomaly detection on Azure, there are 4 options\n","\n","- [Azure Time Series Insights](#1.1)\n","- [Azure Data Explorer](#1.2)\n","- [Custom Code](#1.3)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gsPWolnnJ_RZ"},"source":["### How to run this Notebook on Azure Compute Instance\n","\n","Kindly note, this notebook can be run either on Local Desktop or on an Azure Compute Instance\n","\n","Please follow the below instructions to run this notebook on an Azure Compute Instance.\n","\n","1. Submit an Aeroline AI shared Workspace on-boarding request on the Aeroline AI Portal Site (https://aeroline-ai.azurewebsites.net/development/)\n","2. Once your request has been approved, you will get the access links to the Aeroline AI workspace\n","3. Azure Compute Instances are non-sharable machines meant for individual Data Scientists.\n","4. To copy this notebook to the Workspace, login to the Workspave and click the 'Notebooks' Tab \n","5. Upload your code as shown in below screenshot\n","\n","![image.png](attachment:image.png)\n"]},{"cell_type":"markdown","metadata":{"id":"8tPVlNtsJ_Ra"},"source":["\n","6. Create a new Azure Compute Instance, if it does not exists in first place\n","\n","![image.png](attachment:image.png)\n","\n","7. Notebook can be run once the Compute Instance is up and running.\n","8. There is also an option to do SSH on the compute instance and perform the GIT checkout of the code"]},{"cell_type":"markdown","metadata":{"id":"KUS6CEuBJ_Rb"},"source":["### Anomaly Detection Solution Approach\n","\n","Please use the decision tree to find the correct applicable option :-\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"f3orUM-dJ_Rb"},"source":["<a id='1.1'></a>\n","# 1.1 Azure Time Series Insights\n","\n","\n","Azure Time Series Insights Gen2 is an open and scalable end-to-end IoT analytics service that delivers premium user experiences and rich APIs to integrate its powerful functionality into your existing application or workflow.\n","\n","You can use it to collect, process, store, analyze, and query Internet of Things (IoT) scale data. These data are highly contextualized and optimized for time series.\n","\n","Azure Time Series Insights Gen2 is designed for ad hoc data mining and operational analysis, which allows you to discover hidden trends, pinpoint anomalies, and conduct root cause analysis. It is an open and flexible offer that meets the broad needs of industrial IoT deployments\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"H7t34V5DJ_Rc"},"source":["### 1.1.1 Azure Time Series Insights Use Cases\n","\n","Azure Time Series Insights is suitable for below use cases\n","\n","- Data Exploration and Visual Anomaly Detection\n","- Operational analysis and driving process efficiency\n","- Advanced Analytics\n","\n","These Use cases are explained in details in below cells"]},{"cell_type":"markdown","metadata":{"id":"XM3XdRxbJ_Rd"},"source":["#### 1.1.1.1 Data Exploration and Visual Anomaly Detection\n","\n","Instantly explore and analyze billions of events to spot anomalies and discover hidden trends in your data. Azure Time Series Insights Gen2 delivers near real-time performance for your IoT and DevOps analysis workloads.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"t6BQutmfJ_Rd"},"source":["#### 1.1.1.2 Operational analysis and driving process efficiency\n","\n","Use Azure Time Series Insights Gen2 to monitor the health, usage, and performance of equipment at scale and measure operational efficiency. Azure Time Series Insights Gen2 helps manage diverse and unpredictable IoT workloads without sacrificing ingestion or query performance\n"]},{"cell_type":"markdown","metadata":{"id":"HVStpkqIJ_Rd"},"source":["#### 1.1.1.3 Advanced Analytics\n","\n","Integrate with advanced analytics services such as Machine Learning and Azure Databricks. Azure Time Series Insights Gen2 ingresses raw data from millions of devices. It adds contextual data that can be consumed seamlessly by a suite of Azure analytics services\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"30LdnVGHJ_Re"},"source":["### 1.1.2 Acess to Azure Time Series Insights - (Aeroline AI Development Instance)\n","\n","Kindly contact Admin to get access to Aeroline AI Azure Time Series Development Instance"]},{"cell_type":"markdown","metadata":{"id":"Sf51wCxCJ_Re"},"source":["### 1.1.3 Experiment Using Time Series Insights\n","\n","Please follow the below tutorial link to try out Time Series Insights for Time Series Analysis\n","\n","https://docs.microsoft.com/en-in/azure/time-series-insights/tutorials-set-up-tsi-environment\n","\n","Kindly note: Data needs to be fed either in real time from Azure IOT Hub or from an Simulator App which feeds data into Azure Event Hub\n"]},{"cell_type":"markdown","metadata":{"id":"U6D7EmzNJ_Rf"},"source":["### 1.1.4 Support for Multi-variate Time Series Analysis\n","\n","Azure Time Series Analysis (TSI) fully supports Multi-Variate Time Series - Analysis and Anomaly detection.\n","\n","Below screenshot depicts an example of an Multi-Variate Dataset Model\n","\n","\n","![image.png](attachment:image.png)\n","\n","Please refer to below link for understanding the Input Data Schemas (Model)\n","\n","https://docs.microsoft.com/en-us/azure/time-series-insights/concepts-model-overview"]},{"cell_type":"markdown","metadata":{"id":"GhxD03wJJ_Rf"},"source":["### 1.1.5 Time Series Insights - Final Conclusion\n","\n","This option can be used when you want to perform Anomaly detection on live data being fed from Azure IOT/Event Hub.\n","\n","This options cannot be used for Anomaly detection on Standalone Datasets\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xMUkC8zLJ_Rf"},"source":["<a id='1.2'></a>\n","# 1.2 Time Series Analysis using ADX (Azure Data Explorer)"]},{"cell_type":"markdown","metadata":{"id":"ZnF0C6pXJ_Rg"},"source":["There is another Azure Product (Azure Data Explorer) which can be used to perform Anomaly detection\n","\n","Azure Data Explorer is a fast and highly scalable data exploration service for log and telemetry data. It helps you handle the many data streams emitted by modern software, so you can collect, store, and analyze data. Azure Data Explorer is ideal for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This data is used for diagnostics, monitoring, reporting, machine learning, and additional analytics capabilities. Azure Data Explorer makes it simple to ingest this data and enables you to do complex ad hoc queries on the data in seconds.\n","\n","### 1.2.1 What makes Azure Data Explorer unique?\n","Scales quickly to terabytes of data, in minutes, allowing rapid iterations of data exploration to discover relevant insights.\n","\n","Offers an innovative query language, optimized for high-performance data analytics.\n","\n","Supports analysis of high volumes of heterogeneous data (structured and unstructured).\n","\n","Provides the ability to build and deploy exactly what you need by combining with other services to supply an encompassing, powerful, and interactive data analytics solution.\n","\n","### 1.2.2 Azure Data Explorer workflow\n","Azure Data Explorer integrates with other major services to provide an end-to-end solution that includes data collection, ingestion, storage, indexing, querying, and visualization. It has a pivotal role in the data warehousing flow by executing the EXPLORE step of the flow on terabytes of diverse raw data.\n","\n","![image.png](attachment:image.png)\n","\n","Azure Data Explorer supports several ingestion methods, including connectors to common services like Event Hub, programmatic ingestion using SDKs, such as .NET and Python, and direct access to the engine for exploration purposes. Azure Data Explorer integrates with analytics and modeling services for additional analysis and visualization of data."]},{"cell_type":"markdown","metadata":{"id":"xEMl4YwzJ_Rg"},"source":["### 1.2.3 Azure Data Explorer flow and Tutorial\n","\n","The following diagram shows the different aspects of working with Azure Data Explorer.\n","\n","![image.png](attachment:image.png)\n","\n","Work in Azure Data Explorer generally follows this pattern:\n","\n","#### 1.2.3.1 Create database: \n","Create a cluster and then create one or more databases in that cluster. Quickstart: Create an Azure Data Explorer cluster and database\n","\n","Please follow below link to know how to create cluster and database\n","\n","https://docs.microsoft.com/en-in/azure/data-explorer/create-cluster-database-portal\n","\n","#### 1.2.3.2 Ingest data: \n","Load data into database tables so that you can run queries against it. Quickstart: Ingest data from Event Hub into Azure Data Explorer\n","\n","Please follow below link to know how to ingest data\n","\n","https://docs.microsoft.com/en-in/azure/data-explorer/ingest-sample-data\n","\n","#### 1.2.3.3 Query database: \n","Use our web application to run, review, and share queries and results. It's available in the Azure portal and as a stand-alone application. You can also send queries programmatically (using an SDK) or to a REST API endpoint.\n","\n","Please follow below link to know how to create and run queries\n","\n","https://docs.microsoft.com/en-in/azure/data-explorer/web-query-data\n","\n","#### 1.2.3.4 Query experience\n","A query in Azure Data Explorer is a read-only request to process data and return the results of this processing, without modifying the data or metadata. You continue refining your queries until you've completed your analysis. Azure Data Explorer makes this process easy because of its fast ad hoc query experience.\n","\n","Azure Data Explorer handles large amounts of structured, semi-structured (JSON-like nested types) and unstructured (free-text) data equally well. It allows you to search for specific text terms, locate particular events, and perform metric-style calculations on structured data. Azure Data Explorer bridges the worlds of unstructured text logs and structured numbers and dimensions by extracting values in runtime from free-form text fields. Data exploration is simplified by combining fast text indexing, column store, and time series operations."]},{"cell_type":"markdown","metadata":{"id":"1vbYoWaxJ_Ri"},"source":["<a id='1.2.4'></a>\n","### 1.2.4 EDA using Azure Data Explorer( ADX)\n","\n","ADX is can be used to create and analyze thousands of time series in seconds, enabling near real-time monitoring solutions and workflows\n","\n","#### 1.2.4.1 Time Series Analysis - supported features\n","\n","There are number of features provided by Azure Data Explorer that can be used for Time Series Anomaly detection - EDA\n"]},{"cell_type":"markdown","metadata":{"id":"2aTopDizJ_Rj"},"source":["##### 1.2.4.1.1 Create Time Series\n","\n","In this section, we'll create a large set of regular time series simply and intuitively using the make-series operator, and fill-in missing values as needed. The first step in time series analysis is to partition and transform the original telemetry table to a set of time series. The table usually contains a timestamp column, contextual dimensions, and optional metrics. The dimensions are used to partition the data. The goal is to create thousands of time series per partition at regular time intervals.\n","\n","The input table demo_make_series1 contains 600K records of arbitrary web service traffic. Use the command below to sample 10 records:\n","\n","![image.png](attachment:image.png)\n","\n","Use the make-series operator to create a set of three time series, where:\n","- num=count(): time series of traffic\n","- range(min_t, max_t, 1h): time series is created in 1-hour bins in the time range (oldest and newest timestamps of table records)\n","- default=0: specify fill method for missing bins to create regular time series. Alternatively use series_fill_const(), series_fill_forward(), series_fill_backward() and series_fill_linear() for changes\n","byOsVer: partition by OS\n","The actual time series data structure is a numeric array of the aggregated value per each time bin. We use render timechart for visualization.\n","\n","In the table above, we have three partitions. We can create a separate time series: Windows 10 (red), 7 (blue) and 8.1 (green) for each OS version as seen in the graph:"]},{"cell_type":"markdown","metadata":{"id":"UW34xhT0J_Rk"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"a1GgTPr7J_Rl"},"source":["\n","##### 1.2.4.1.2 Filtering\n","Filtering is a common practice in signal processing and useful for time series processing tasks (for example, smooth a noisy signal, change detection).\n","\n","There are two generic filtering functions:\n","\n","- series_fir(): Applying FIR filter. Used for simple calculation of moving average and differentiation of the time series for change detection.\n","\n","- series_iir(): Applying IIR filter. Used for exponential smoothing and cumulative sum.\n","Extend the time series set by adding a new moving average series of size 5 bins (named ma_num) to the query:\n","\n","![image.png](attachment:image.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wJEufJXNJ_Rl"},"source":["#### 1.2.4.1.2 Regression analysis\n","ADX supports segmented linear regression analysis to estimate the trend of the time series.\n","\n","- Use series_fit_line() to fit the best line to a time series for general trend detection.\n","- Use series_fit_2lines() to detect trend changes, relative to the baseline, that are useful in monitoring scenarios.\n","Example of series_fit_line() and series_fit_2lines() functions in a time series query:\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"FeUqCy2-J_Rm"},"source":["##### 1.2.4.1.3 Seasonality detection\n","Many metrics follow seasonal (periodic) patterns. User traffic of cloud services usually contains daily and weekly patterns that are highest around the middle of the business day and lowest at night and over the weekend. IoT sensors measure in periodic intervals. Physical measurements such as temperature, pressure, or humidity may also show seasonal behavior.\n","\n","The following example applies seasonality detection on one month traffic of a web service (2-hour bins):\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"KH-6ZBnUJ_Rm"},"source":["![image.png](attachment:image.png)\n","\n","The function detects daily and weekly seasonality. The daily scores less than the weekly because weekend days are different from weekdays."]},{"cell_type":"markdown","metadata":{"id":"XT8KNKP0J_Rn"},"source":["##### 1.2.4.1.4 Element wise operations\n","Arithmetic and logical operations can be done on a time series. Using series_subtract() we can calculate a residual time series, that is, the difference between original raw metric and a smoothed one, and look for anomalies in the residual signal:\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"kyISQJnsJ_Rn"},"source":["<a id='1.3'></a>\n","# 1.3 Custom Code\n","\n","If the none of the above 3 options suits your requirements, then there is a 3rd option to Create a Custom Code Jupyter Notebook for performing forecasting.\n","\n","This section of Notebook can run either on Local Workstation or on a Azure Compute Instance"]},{"cell_type":"markdown","metadata":{"id":"aXO7jen5J_Rn"},"source":["### 1.3.1 Import Libraries"]},{"cell_type":"markdown","metadata":{"id":"N3K4o-osJ_Rn"},"source":["We will need:\n","- Numpy and Pandas to work with tabular data\n","- Matplotlib, Ipywidgets for data visualisation \n","- StatsModels for exporing time series properties and forecasting\n"]},{"cell_type":"code","metadata":{"id":"nY6fLh1DJ_Ro","executionInfo":{"status":"ok","timestamp":1612246782983,"user_tz":-330,"elapsed":4178,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["import warnings\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    import numpy as np\n","    import pandas as pd\n","    # import bcolz\n","    import seaborn as sns\n","    import os\n","    import matplotlib.pyplot as plt\n","    from itertools import product\n","    import time\n","\n","%matplotlib inline\n","data_path = 'input/'\n","seed=1204\n","\n","def get_submission(test,item_cnt_month):\n","    sub = test.copy()\n","    sub['item_cnt_month'] = item_cnt_month\n","    sub.drop(['item_id','shop_id'],axis=1,inplace=True)\n","    sub.to_csv(data_path + 'submission.csv',index=False)\n","    return sub"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBqeOvqJJ_Rp"},"source":["### 1.3.2 Import Libraries useful for Modeling"]},{"cell_type":"code","metadata":{"id":"AAGmsqajJ_Rq","executionInfo":{"status":"ok","timestamp":1612246784339,"user_tz":-330,"elapsed":3627,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["import os\n","import pandas as pd\n","import sklearn\n","from sklearn.ensemble import IsolationForest\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fdu4UXZQJ_Rq"},"source":["### 1.3.3 Import the Raw Dataset\n","\n","We would be using the 'Future-sales-prediction' from Kaggle for this Experiment\n","https://www.kaggle.com/c/competitive-data-science-final-project \n","\n","Objective : Data contains daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. \n","Note - The list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n","\n","### Briefing of the Dataset\n","#### File descriptions:\n","- sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n","- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n","- sample_submission.csv - a sample submission file in the correct format.\n","- items.csv - supplemental information about the items/products.\n","- item_categories.csv  - supplemental information about the items categories.\n","- shops.csv- supplemental information about the shops.\n","\n","#### Data fields:\n","\n","- ID - an Id that represents a (Shop, Item) tuple within the test set\n","- shop_id - unique identifier of a shop\n","- item_id - unique identifier of a product\n","- item_category_id - unique identifier of item category\n","- item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n","- item_price - current price of an item\n","- date - date in format dd/mm/yyyy\n","- date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","- item_name - name of item\n","- shop_name - name of shop\n","- item_category_name - name of item category\n","Note:- Kindly download the dataset from the above link to this folder"]},{"cell_type":"markdown","metadata":{"id":"0ZS3a9xeJ_Rq"},"source":["#### 1.3.3.1 Load data "]},{"cell_type":"code","metadata":{"id":"hR2xkSFQJ_Rr","executionInfo":{"status":"ok","timestamp":1612246788490,"user_tz":-330,"elapsed":4183,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["transactions    = pd.read_csv(os.path.join(data_path, 'sales_train.csv.gz'))\n","items           = pd.read_csv(os.path.join(data_path, 'items.csv'),encoding ='ISO-8859-1')\n","item_categories = pd.read_csv(os.path.join(data_path, 'item_categories.csv'),encoding ='ISO-8859-1')\n","shops           = pd.read_csv(os.path.join(data_path, 'shops.csv'),encoding ='ISO-8859-1')\n","test            = pd.read_csv(os.path.join(data_path, 'test.csv.gz'))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MP7VR3ShJ_Rr"},"source":["#### 1.3.3.2 Data distribution"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7u4LFdfBJ_Rr","executionInfo":{"status":"ok","timestamp":1612246796763,"user_tz":-330,"elapsed":3848,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}},"outputId":"b2ea30a7-98ae-4c49-ad9a-c9045e6fab5d"},"source":["print(transactions.shape)\n","print(test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(2935849, 6)\n","(214200, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_tve9z_mJ_Rs","executionInfo":{"status":"ok","timestamp":1612246796764,"user_tz":-330,"elapsed":3324,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["transactions['date_format'] = pd.to_datetime(transactions.date,format='%d.%m.%Y')\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cp8VFJvcJ_Rs"},"source":["<a id='1.3.4'></a>\n","### 1.3.4 Exploration and Analysis of Data:\n","\n","Exploratory Data Analysis or (EDA) is understanding the data sets by summarizing their main characteristics often plotting them visually. It is  ssentially a type of storytelling for statisticians which allows us to uncover patterns and insights, often with visual methods, within data.This step is very important especially when we arrive at modeling the data in order to apply Machine learning.</br>\n","\n","We will explore a Data set and perform the exploratory data analysis. The major topics to be covered are below:\n","- Remove unwanted columns\n","- Count of unique values\n","- And, Check Summary stats</br>"]},{"cell_type":"markdown","metadata":{"id":"-7s6nFTfJ_Rs"},"source":["Basic data analysis is done, including plotting sum and mean of item_cnt_day for each month to find some patterns, exploring missing values, inspecting test set."]},{"cell_type":"markdown","metadata":{"id":"YYTFFG4eJ_Rs"},"source":["#### 1.3.4.1 Distribution of data"]},{"cell_type":"code","metadata":{"id":"_PO1P0P9J_Rs"},"source":["transactions.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1iC8x76J_Rt"},"source":["transactions.item_cnt_day.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bqg4oDhXJ_Rt"},"source":["#### 1.3.4.2 Sum of items count per month"]},{"cell_type":"code","metadata":{"id":"qpD3-ReGJ_Rt"},"source":["plt.style.use('seaborn')\n","\n","transactions.copy().set_index('date_format').item_cnt_day.resample('M') \\\n","                            .sum().plot();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pkiu3e2cJ_Rt"},"source":["#### 1.3.4.3 Mean of items count per month"]},{"cell_type":"code","metadata":{"id":"ta_M5ZOsJ_Ru"},"source":["plt.style.use('seaborn')\n","transactions.copy().set_index('date_format').item_cnt_day.resample('M') \\\n","                            .mean().plot();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xy2kH06LJ_Ru"},"source":["#### 1.3.4.4 Sum and mean of October items count"]},{"cell_type":"code","metadata":{"id":"S91pBkCHJ_Ru"},"source":["# get oct sales per each day\n","plt.style.use('seaborn')\n","transactions[list(map(lambda x: x.month in [10] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .sum().plot();\n","plt.ylim(0, 9000)\n","plt.show()\n","transactions[list(map(lambda x: x.month in [10] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .mean().plot();\n","plt.ylim(0, 2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aW8oLFJ8J_Ru"},"source":["#### 1.3.4.5 Sum and mean of November items count"]},{"cell_type":"code","metadata":{"id":"yo4PpxAAJ_Ru"},"source":["# get nov sales per each day\n","plt.style.use('seaborn')\n","transactions[list(map(lambda x: x.month in [11] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .sum().plot();\n","plt.ylim(0, 9000)\n","plt.show()\n","transactions[list(map(lambda x: x.month in [11] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .mean().plot();\n","plt.ylim(0, 2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llIZpzgxJ_Rv"},"source":["#### 1.3.4.6 Sum and mean of December items count"]},{"cell_type":"code","metadata":{"id":"xLX_oWfBJ_Rv"},"source":["\n","# get dec sales per each day\n","plt.style.use('seaborn')\n","transactions[list(map(lambda x: x.month in [12] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .sum().plot();\n","plt.ylim(0, 9000)\n","plt.show()\n","transactions[list(map(lambda x: x.month in [12] , transactions.date_format))].set_index('date_format').item_cnt_day.resample('D') \\\n","                            .mean().plot();\n","plt.ylim(0, 2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76tubbq_J_Rv"},"source":["#### Summary:\n","Here are few things interesting I found from doing EDA:\n","\n","- Number of sold items declines over the year\n","- There are peaks in November and similar item count behaviors in June-July-August. This inspires to look up Russia national holiday and create a Boolean holiday features. More information can be found in ‘Feature Engineering’ section\n","- Data has no missing values\n","\n","##### Some interesting information from test set analysis:\n","- Not all shop_id in training set are used in test set. Test set excludes following shops (but not vice versa): [0, 1, 8, 9, 11, 13, 17, 20, 23, 27, 29, 30, 32, 33, 40, 43, 51, 54]\n","- Not all item in train set are in test set and vice versa\n","- In test set, a fixed set of items (5100) are used for each shop_id, and each item only appears one per each shop. This possibly means that items are picked from a generator, which will result in lots of 0 for item count. Therefore, generating all possible shop-item pairs for each month in train set and assigning missing item count with 0 makes sense."]},{"cell_type":"markdown","metadata":{"id":"rHEl8v87J_Rv"},"source":["### 1.3.5 Individual Feature Visualization"]},{"cell_type":"markdown","metadata":{"id":"FFY5Vo3HJ_Rw"},"source":["## Time-series visualization of the sales"]},{"cell_type":"markdown","metadata":{"id":"NAksBPSaJ_Rw"},"source":["#### 1.3.5.1 sales on  Yearly basis\n","Let us see how sales of a given item in a given store varies in a span of 5 years."]},{"cell_type":"markdown","metadata":{"id":"7mTvjVyJJ_Rw"},"source":["#### 1.3.5.2 Aggregate the sales on a montly basis\n","Let us make it more interesting. What if we aggregate the sales on a montly basis and compare different items and stores.   \n","This should help us understand how different item sales behave at a high level."]},{"cell_type":"markdown","metadata":{"id":"iqL4jWVaJ_Rw"},"source":["#### 1.3.5.3 Interactive visualization "]},{"cell_type":"markdown","metadata":{"id":"FLzg6R-1J_Rx"},"source":["Let's plot sales time series to get some general ideas about the data. The data is averaged (weekly) to make plots more readable.\n","\n","Time series look quite regular, with no obvious outliers. Sales volumes across different items and stores definitely have similatities. In particular, the sales have a trend and a yearly sesonality pattern with a spike during the summer.\n","\n","We can also see that sales of one item or in one store are definitely correlated. "]},{"cell_type":"markdown","metadata":{"id":"lb74fpgGJ_Rx"},"source":["### 1.3.6 Time series Decomposition:\n","Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components.\n","\n","Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting</br>\n","\n","These components are defined as follows:\n","\n","* Level: The average value in the series.\n","* Trend: The increasing or decreasing value in the series.\n","* Seasonality: The repeating short-term cycle in the series.\n","* Noise: The random variation in the series."]},{"cell_type":"markdown","metadata":{"id":"9Cw1mvSRJ_Rx"},"source":["#### 1.3.6.1 Trend and seasonality\n","Let's analyze sales trends and seasonalities. First, we add columns with some date-related information: year, month, weekday, etc."]},{"cell_type":"markdown","metadata":{"id":"jllR9KYAJ_Rx"},"source":["#### 1.3.6.2 Aggregared data view: \n","sales time series averaged across all items and stores. At first glance, it may look like aggregated time series is much more turbulent than individual ones. In reality, it's not the case: I just plotted it without weekly averaging.\n","\n","We can see clear yearly and weekly seasonality patterns, which don't really change with time. This is definitely interesting, and also confirms our hypothesis that the sales could be correlated. In the meanwhile, monthly seasonality is not very pronounced."]},{"cell_type":"markdown","metadata":{"id":"zhNDYru9J_Rx"},"source":["There is also a clear upward trend.\n","The trend is almost linear, with higher than average increase in 2014 and lower in 2017."]},{"cell_type":"markdown","metadata":{"id":"PWpVWMi4J_Ry"},"source":["#### 1.3.6.3 Volatility\n","\n","For sales data we are like to observe periods with high and low volatilites. For example, periods before holidays or certain seasons (depending on what kind of items we have) may have higher customer turn-over.\n","\n","We check standard deviation of sales volumes across all stores and items, computed independantly at every day (and normalized by average sales volume for every item and store). This plot is almost exactly the same as the plot with aggregated sales above: meaning, periods with higher sales have higher dispersion across different items and stores. The match between these two plot is really good, but standard deviation is \"flatter\". It indicates that \"noise\" in sales data is somewhere between multiplicative and additive. Let's keep that in mind.\n","\n","30-day rolling volatility shows a similar picture. No new insights, except that this plot seems to be consistent with what we've seen so far. "]},{"cell_type":"markdown","metadata":{"id":"mpkwunU0J_Ry"},"source":["#### 1.3.6.4 STL decomposition\n","\n","We so a rather strong evidence that sales have both trend and seasonality components. As the last step in our EDA, let's apply classical [STL decomposition](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) to individual sales time series. \n","\n","We plot trend, seasonal and residual components of the STL decomposition. For for the residuals, partial autocorrelation (PACF) is also plotted; in the title, you can see a p-value of Ljung-Box test for 7 and 30 lags. P-value <0.05 means that the residuals are likely to be autocorrelated, and there is still some information to be extracted.\n","\n","It seems yearly seasonality is a must: without it, our trend becomes increadibly noisy. However, just yearly seasonality is not sufficient: the PACF plot shows a weekly seasonality pattern in residuals, and Ljung-Box test fails badly. Thus, both annual and weekly seasonalities should be modelled. \n","\n","We also notice that multiplicative STL fits data better - but only marginally so."]},{"cell_type":"markdown","metadata":{"id":"F5wpvmN_J_Ry"},"source":["### 1.3.5 Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"f3aROSWDJ_Ry"},"source":["#### Preparing our data\n","Since our goal is to predict monthly sales, we need to first set up a dataframe that has the data aggregated by month. The result is a grid which we then merge with aggregated data (sum of item_cnt_day) to form our \"all_data\" dataframe with our target variable."]},{"cell_type":"code","metadata":{"id":"wsTZ7VbGJ_Rz","executionInfo":{"status":"ok","timestamp":1612246810664,"user_tz":-330,"elapsed":3629,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["transactions = transactions[transactions.item_cnt_day<=1000] # there is only 1 item\n","transactions = transactions[transactions.item_price<100000]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLVp11WGJ_Rz","executionInfo":{"status":"ok","timestamp":1612246826942,"user_tz":-330,"elapsed":18708,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# Create \"grid\" with columns\n","index_cols = ['shop_id', 'item_id', 'date_block_num']\n","\n","# For every month we create a grid from all shops/items combinations from that month\n","grid = [] \n","for block_num in transactions['date_block_num'].unique():\n","    cur_shops = transactions.loc[transactions['date_block_num'] == block_num, 'shop_id'].unique()\n","    cur_items = transactions.loc[transactions['date_block_num'] == block_num, 'item_id'].unique()\n","    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n","\n","# Turn the grid into a dataframe\n","grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKNHlwDEJ_Rz","executionInfo":{"status":"ok","timestamp":1612246831585,"user_tz":-330,"elapsed":18540,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# create target\n","gb = transactions.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n","\n","# merge with items\n","all_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n","all_data.rename(columns = {'item_cnt_day':'target'},inplace=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tYRrWrMJ_Rz","executionInfo":{"status":"ok","timestamp":1612246833137,"user_tz":-330,"elapsed":15804,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# merge with category id\n","all_data =pd.merge(all_data,items,on=['item_id'],how='left')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeeKjNw5J_Rz","executionInfo":{"status":"ok","timestamp":1612246836322,"user_tz":-330,"elapsed":17946,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# Same as above but with shop-month aggregates\n","gb = transactions.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'sum','mean'}})\n","gb.columns = ['shop_id', 'date_block_num','shop_block_target_sum',\"item_block_target_mean\"]\n","all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"cy9uw7_yJ_R0","executionInfo":{"status":"ok","timestamp":1612246839888,"user_tz":-330,"elapsed":18141,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# Same as above but with item-month aggregates\n","gb = transactions.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'sum','mean'}})\n","gb.columns = ['item_id', 'date_block_num','item_block_target_sum',\"item_block_target_mean\"]\n","all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjqgxHvuJ_R0","executionInfo":{"status":"ok","timestamp":1612246842974,"user_tz":-330,"elapsed":20106,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# Same as above but with item category-month aggregates\n","sales =pd.merge(transactions,items,on=['item_id'],how='left')\n","gb = sales.groupby(['item_category_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'sum','mean'}})\n","gb.columns = ['item_category_id', 'date_block_num','item_cat_block_target_sum',\"item_cat_block_target_mean\"]\n","all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"iPGZce3RJ_R0","executionInfo":{"status":"ok","timestamp":1612246842976,"user_tz":-330,"elapsed":18598,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["def downcast_dtypes(df):\n","    '''\n","        Changes column types in the dataframe: \n","                \n","                `float64` type to `float32`\n","                `int64`   type to `int32`\n","    '''\n","    \n","    # Select columns to downcast\n","    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n","    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n","    \n","    # Downcast\n","    df[float_cols] = df[float_cols].astype(np.float32)\n","    df[int_cols]   = df[int_cols].astype(np.int32)\n","    \n","    return df"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e7pYSAuJ_R0","executionInfo":{"status":"ok","timestamp":1612246842977,"user_tz":-330,"elapsed":17489,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["# Downcast dtypes from 64 to 32 bit to save memory\n","all_data = downcast_dtypes(all_data)\n","test = downcast_dtypes(test)\n","del grid, gb "],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiMHwW_kJ_R1","executionInfo":{"status":"ok","timestamp":1612246843474,"user_tz":-330,"elapsed":16401,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}},"outputId":"51d849eb-0f9b-4711-9879-6f263992141b"},"source":["all_data.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10913804, 12)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"l9htJeU1J_R1","executionInfo":{"status":"ok","timestamp":1612246961379,"user_tz":-330,"elapsed":132763,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}}},"source":["all_data.to_csv(data_path+'new_sales.csv',index=False)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvKJtjH-J_R1"},"source":["#### Generate lag feature new_sales_lag_after12.pickle"]},{"cell_type":"code","metadata":{"id":"f1KX_3uBJ_R1"},"source":["all_data = pd.read_csv(data_path+'new_sales.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NahAbD6qJ_R2","executionInfo":{"status":"ok","timestamp":1612249407118,"user_tz":-330,"elapsed":1398,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}},"outputId":"7caeda91-fbaf-42ec-c692-75b5c33e7a9c"},"source":["index_cols = ['shop_id', 'item_id', 'date_block_num','item_category_id']\n","cols_to_rename = list(all_data.columns.difference(index_cols))\n","for i in ['item_name']:\n","    cols_to_rename.remove(i)\n","print(cols_to_rename)\n","cols_gb_item = [i for i in cols_to_rename if 'item_block' in i]\n","cols_gb_shop = [i for i in cols_to_rename if 'shop_block' in i]\n","cols_gb_cat = [i for i in cols_to_rename if 'item_cat' in i]\n","cols_gb_all = ['target']\n","cols_gb_key=[['item_id'],['shop_id'],['item_category_id'],['shop_id','item_id']]\n","cols_gb_value = [cols_gb_item,cols_gb_shop,cols_gb_cat,cols_gb_all]\n","print(cols_gb_value)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["['item_block_target_mean_x', 'item_block_target_mean_y', 'item_block_target_sum', 'item_cat_block_target_mean', 'item_cat_block_target_sum', 'shop_block_target_sum', 'target']\n","[['item_block_target_mean_x', 'item_block_target_mean_y', 'item_block_target_sum'], ['shop_block_target_sum'], ['item_cat_block_target_mean', 'item_cat_block_target_sum'], ['target']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KadkQmUxJ_R3"},"source":["shift_range = [1,2,3,5]\n","for month_shift in shift_range:\n","    for k,v in zip(cols_gb_key,cols_gb_value): \n","        index_col = ['date_block_num'] + k\n","        train_shift = all_data[index_col + v].copy().drop_duplicates()\n","\n","        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n","\n","        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in v else x\n","        train_shift = train_shift.rename(columns=foo)\n","        all_data = pd.merge(all_data, train_shift, on=index_col, how='left').fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kLdlP0fJ_R4"},"source":["all_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEcWDmY4J_R5"},"source":["all_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YZ3g12XJ_R5"},"source":["#### save to disk\n","#all_data.to_csv(data_path+'new_sales_lag.csv',index=False)"]},{"cell_type":"code","metadata":{"id":"WJiK1aA9J_R5"},"source":["all_data = all_data[all_data['date_block_num'] >= 12]\n","all_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwZlrAMLJ_R6"},"source":["# all_data.to_csv(data_path+'new_sales_lag_after12.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rk_DCHqJ_R6"},"source":["#### Add boolean holiday features\n"]},{"cell_type":"code","metadata":{"id":"qOSmjok8J_R6"},"source":["all_data['December'] = all_data.date_block_num.apply(lambda x: 1 if x ==23 else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQr1uaJuJ_R6"},"source":["all_data['Newyear_Xmas'] = all_data.date_block_num.apply(lambda x: 1 if x in [12,24] else 0)\n","all_data['Valentine_MenDay'] = all_data.date_block_num.apply(lambda x: 1 if x in [13,25] else 0)\n","all_data['WomenDay'] = all_data.date_block_num.apply(lambda x: 1 if x in [14,26] else 0)\n","all_data['Easter_Labor'] = all_data.date_block_num.apply(lambda x: 1 if x in [15,27] else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2VnJSGHJ_R6"},"source":["all_data.to_pickle(data_path+'new_sales_lag_after12.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNrYEmgDJ_R7"},"source":["all_data = pd.read_pickle(data_path+'new_sales_lag_after12.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-409UkQYJ_R7"},"source":["all_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBqgp0cZJ_R7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtOOpOxKJ_R7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kknhF10ZJ_R8"},"source":["### 1.3.7 Time Series Forecasting  - Algorithm Selection\n","\n","For Solving Forecasting Problem, there are multiple ML Alogorithms to choose from \n","\n","Please refer to below table to choose the appropriate ML algorithm\n","\n","![image.png](attachment:image.png)\n","\n","Please find the descriptiom of some of these key algorithms:-"]},{"cell_type":"markdown","metadata":{"id":"cey-OGrNJ_R8"},"source":["#### 1.3.7.1 ARIMA\n","\n","ARIMA is Autoregressive Integrated Moving Average Model, which is a component of SARIMAX, i.e. Seasonal ARIMA with eXogenous regressors.\n","\n","(sources: [1](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/), [2](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3), [3](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases))\n","\n","\n","http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"]},{"cell_type":"markdown","metadata":{"id":"QcYJtlH2J_R8"},"source":["#### 1.3.7.2 LSTM — Supervised\n","LSTM is a supervised method that, given a time series sequence as input, predicts the value at the next timestamp.\n","\n","Training LSTM\n","Using features for all storeitems (stacked sales from previous day) to predict sales for one storeitem (sales for current day).\n","![LSTM.png](attachment:LSTM.png)"]},{"cell_type":"markdown","metadata":{"id":"I8DGfnj_J_R8"},"source":["#### 1.3.7.3 Lightgbm — Supervised\n","LSTM is a supervised method that, given a time series sequence as input, predicts the value at the next timestamp.m\n","\n","![lightgbm.JPG](attachment:lightgbm.JPG)"]},{"cell_type":"markdown","metadata":{"id":"o7okUFkbJ_R8"},"source":["#### 1.3.7.4 Fbprophet"]},{"cell_type":"markdown","metadata":{"id":"FTUWE84qJ_R9"},"source":["Now, let's test Facebook Prophet package. Like statsmodels STL, Prophet also uses a trend-seasonality decomposition; however, it is more flexible and allows making predictions. Under the hood, Prophet solves a state space model using the Bayesian framework of [Stan](http://mc-stan.org/).\n","\n","**Pros:**\n","- Quite easy to use\n","- Allows specifying multiple seasonalities\n","- Allows specifying special events\n","- Can compute quick MAP and slow but accurate Bayesian estimates\n","- Provides methods for basic plotting out of the box\n","\n","**Cons:**\n","- Can only treat univariate time series\n","- Assumes Gaussian priors\n","- Does not provide methods to tune hyper-parameters out of the box (for example, seasonality and trend flexibility priors)\n","- Outputs useless warning messages in the console - and I found no way to hide them"]},{"cell_type":"markdown","metadata":{"id":"7jVYi1O_J_SA"},"source":["We are are selecting the 'Fbprophet' Alogrithm for our Notebook Example"]},{"cell_type":"markdown","metadata":{"id":"siIE0B6fJ_SB"},"source":["#### 1.3.8 Implementaion Fbprophet:\n","\n","Prophet here is fit on 2013 - 2016 data with annual and weekly seasonalities using additive decomposition. The latest version of the package allows selecting multiplicative decomposition.\n","\n","In any case, the fit looks reasonable. Trend and seasonalities of sales across stores and items look similar and consistent with our previous analysis. We see that Prophet adjusted the trend quite a bit in 2014 - well, we saw the trend in 2014 was kind of an outiler on the aggregate level. We see almost zero uncertainty in the trend component, which is probably good (uncertainty in the seasonal component is not plotted). \n","\n","Partial autocorrelation plot of the predicted residuals looks much better than the one for STL decomposition. Ljung-Box tests is doing definitely better as well; p-value for monthly lags is a bit low but still acceptable.\n","\n","I also output SMAPE over time for the training (2013 - 2016) and validation (2017 Q1) data, smoothed using LOWESS for better visibility. On average, SMAPE is around 15 and 16 for the training and validation data\n","\n","Overall, the results look okay."]},{"cell_type":"markdown","metadata":{"id":"7qJ8A8q9J_SB"},"source":["#### 1.3.8.1 Example:Building the fbprophet model for store = 1, product = 1 with plot in Bokeh graph"]},{"cell_type":"markdown","metadata":{"id":"QYyVGLI-J_SB"},"source":["Uncomment below code to plot & predict Fbprophet model without bokeh interactive plot."]},{"cell_type":"code","metadata":{"id":"T_2cKWizJ_SB"},"source":["# import warnings\n","# warnings.filterwarnings('ignore')\n","\n","# s1i1 = df_train[(df_train[\"store\"]==1) & (df_train[\"item\"])==1]\n","# s1i1[\"sales\"] = np.log1p(s1i1[\"sales\"])\n","# stats = s1i1[[\"date\", \"sales\"]]\n","# stats.columns = [\"ds\", \"y\"]\n","\n","# m = Prophet()\n","# m.fit(stats)\n","# future = m.make_future_dataframe(periods=365)\n","\n","# forecast = m.predict(future)\n","# forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n","\n","# fig1 = m.plot(forecast)\n","\n","# fig2 = m.plot_components(forecast)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gT1z9c4aJ_SC"},"source":["#### 1.3.8.2 SMAPE - the official metric for the submission\n","Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors. It is usually defined as follows:\n","![image.png](attachment:image.png)\n","\n","where At is the actual value and Ft is the forecast value.\n","\n","The absolute difference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n","\n","other timesereies performance measures:- https://machinelearningmastery.com/time-series-forecasting-performance-measures-with-python/"]},{"cell_type":"code","metadata":{"id":"mhq4VJSpJ_SC"},"source":["\n","def smape(y: Union[np.ndarray, float], yhat: Union[np.ndarray, float], average=True, signed=False) -> float:\n","    \"\"\"SMAPE evaluation metric\"\"\"\n","    \n","    if signed:\n","        result = 2. * (yhat - y) / (np.abs(y) + np.abs(yhat)) * 100\n","    else:\n","        result = 2. * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat)) * 100\n","    if average: return np.mean(result)\n","    return result\n","\n","def smape_df(df: pd.DataFrame, average=True, signed=False) -> pd.DataFrame:\n","    return smape(df.y, df.yhat, average=average, signed=signed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwMEBXGGJ_SC"},"source":["# -- TODO: Add Ipywidgets?\n","\n","def prophet_show(item, store, cutoff_train, cutoff_eval, prophet_kwargs, title, \n","                 plot_components=True, display_df=True):\n","    ts = (df_train.query('item == @item & store == @store & date < @cutoff_eval')[['date', 'sales']]\n","          .rename(columns={'date':'ds', 'sales':'y'})).reset_index(drop=True)\n","    ind_train = pd.eval('ts.ds < cutoff_train')\n","    ind_eval = ~ ind_train\n","    len_train, len_eval = ind_train.sum(), ind_eval.sum()\n","    ts_train = ts.loc[ind_train]\n","    m = Prophet(**prophet_kwargs)\n","    m.fit(ts_train)\n","    ts_hat = m.predict(ts).merge(ts[['ds', 'y']], on='ds', how='left')\n","    if display_df: display(ts_hat.tail(3))\n","\n","    df_combined = ts_hat.assign(smape=0, smape_smooth=0)\n","    df_combined.smape = smape_df(df_combined, average=False)\n","    df_combined.loc[ind_train, 'smape_smooth'] = lowess(df_combined.loc[ind_train, 'smape'], range(len_train), frac=0.03, return_sorted=False)\n","    df_combined.loc[ind_eval, 'smape_smooth'] = lowess(df_combined.loc[ind_eval, 'smape'], range(len_eval), frac=0.35, return_sorted=False)\n","    smape_in = df_combined.loc[ind_train].smape.mean()\n","    smape_oos = df_combined.loc[ind_eval].smape.mean()\n","    \n","    source = ColumnDataSource(data=df_combined)\n","    p = figure(plot_width=750, plot_height=200, title=(\"**{}**     item = {} store = {}     train / test = ..{} / ..{}\"\n","                                                       .format(title, item, store, cutoff_train, cutoff_eval)), \n","               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","    _ = p.line(x='ds', y='yhat', source=source)\n","    _ = p.line(x='ds', y='yhat_lower', source=source, line_alpha=0.4)\n","    _ = p.line(x='ds', y='yhat_upper', source=source, line_alpha=0.4)\n","    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n","    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n","       \n","    deltas = np.abs(m.params['delta'][0])\n","    delta_max = np.max(deltas)\n","    df_deltas = pd.DataFrame({'ds': m.changepoints.values, 'delta':deltas, 'delta_scaled':ts_hat.yhat.mean() * deltas / delta_max})\n","    source2 = ColumnDataSource(df_deltas)\n","    cp1 = p.vbar(x='ds', source=source2, width=1, top=ts_hat.yhat.mean(), color='red', alpha=0.2, hover_color='red', hover_alpha=1)\n","    cp2 = p.vbar(x='ds', source=source2, width=1.5e+9, top='delta_scaled', color='red', alpha=0.5)\n","    p.add_tools(HoverTool(tooltips=[('trend delta', '@delta{.000}')], renderers=[cp2], mode='mouse'))\n","    # p.add_layout(Label(x=1e+10, y=10, text='xasfdfsdfsd'))\n","    p.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n","    \n","    p2 = figure(plot_width=750, plot_height=100, title=\"SMAPE IS / OOS = {:.3f} / {:.3f}\".format(smape_in, smape_oos), x_axis_type='datetime', tools=\"\",\n","                x_range=p.x_range)\n","    sm1 = p2.line(x='ds', y='smape_smooth', source=source, color='green')\n","    p2.add_tools(HoverTool(tooltips=[('smape', '@smape')], renderers=[sm1], mode='vline', line_policy='interp'))\n","    p2.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n","    p2.yaxis[0].ticker.desired_num_ticks = 2\n","    bokeh.io.show(bokeh.layouts.column(p, p2))\n","    \n","    if plot_components:\n","        _ = m.plot_components(ts_hat, uncertainty=True)\n","        fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n","#         res = ts_hat.query('ds < @cutoff_train').yhat - ts_train.y\n","        res = (df_combined['y'] - df_combined['yhat'])\n","#         adfuller_stat = statsmodels.tsa.stattools.adfuller(res.values)\n","        ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res.values)\n","        _ = statsmodels.graphics.tsaplots.plot_pacf(res, lags=40, ax=ax,\n","                                                    title='residuals pacf; ljung-box p-value = {:.2E} / {:.2E}'.format(ljungbox_stat[1][6], \n","                                                                                                                      ljungbox_stat[1][30]))\n","    \n","prophet_show(item=1, store=1, cutoff_train=\"2017-01-01\", cutoff_eval=\"2017-04-01\",\n","             prophet_kwargs={'yearly_seasonality':True, 'weekly_seasonality':True,\n","                            'uncertainty_samples':500},\n","            title='Prophet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVEEc0H-J_SC"},"source":["#### 1.3.8.3 Include holiday effects"]},{"cell_type":"markdown","metadata":{"id":"u6c3Va2jJ_SD"},"source":["We see sales drop from Sunday to Monday so there must be a holiday effect in the sales data. There's a peak in July so this may be due to seasonal sales or summer festivities.\n","\n","Next step, try excluding change points and include holiday effects. Let's count NFL playoffs as a holiday. And add an extra regressor for NFL sundays."]},{"cell_type":"code","metadata":{"id":"aL3OsLZAJ_SD"},"source":["playoffs = ['2013-07-12', '2014-07-12', '2014-07-19',\n","                 '2014-07-02', '2015-07-11', '2016-07-17',\n","                 '2016-07-24', '2016-07-07','2016-07-24']\n","superbowl = ['2013-01-01', '2013-12-25', '2014-01-01', '2014-12-25','2015-01-01', '2015-12-25','2016-01-01', '2016-12-25',\n","                '2017-01-01', '2017-12-25']\n","\n","playoffs = pd.DataFrame({\n","  'holiday': 'playoff',\n","  'ds': pd.to_datetime(playoffs),\n","  'lower_window': 0,\n","  'upper_window': 1,\n","})\n","superbowls = pd.DataFrame({\n","  'holiday': 'superbowl',\n","  'ds': pd.to_datetime(superbowl),\n","  'lower_window': 0,\n","  'upper_window': 1,\n","})\n","holidays = pd.concat((playoffs, superbowls))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Xn_vGO2J_SD"},"source":["s1i1[\"dow\"] = pd.to_datetime(s1i1[\"date\"]).dt.day_name() # day of week\n","s1i1.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMFcqb2MJ_SD"},"source":["def nfl_sunday(ds):\n","    date = pd.to_datetime(ds)\n","    if date.weekday() == 6 and (date.month > 8 or date.month < 2):\n","        return 1\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rd9Xkm_KJ_SD"},"source":["stats = s1i1[[\"date\", \"sales\"]]\n","stats.columns = [\"ds\", \"y\"]\n","stats.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeoUrb0IJ_SE"},"source":["stats[\"nfl_sunday\"] = stats['ds'].apply(nfl_sunday)\n","stats.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UIee8eoJ_SE"},"source":["#### 1.3.8.4 Calling prophet model with holidays effect"]},{"cell_type":"code","metadata":{"id":"CPjYM97yJ_SE"},"source":["m = Prophet(holidays=holidays, holidays_prior_scale=0.5,\n","            yearly_seasonality=4,  interval_width=0.95,\n","            changepoint_prior_scale=0.006, daily_seasonality=True)\n","m.add_regressor('nfl_sunday')\n","m.add_seasonality(name='daily', period=60, fourier_order=5)\n","m.fit(stats)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fLnaPbIJ_SE"},"source":["future = m.make_future_dataframe(periods=90, freq=\"D\") # Daily frequency\n","future['nfl_sunday'] = future['ds'].apply(nfl_sunday)\n","future.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T5FmEdTkJ_SF"},"source":["#### 1.3.8.5 forecasting prophet result"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"IBlhZw05J_SF"},"source":["forecast = m.predict(future)\n","forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxUpGy4xJ_SF"},"source":["#### 1.3.8.6 ploting result with prophet"]},{"cell_type":"code","metadata":{"id":"f42yFCVvJ_SF"},"source":["fig1 = m.plot(forecast)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOoTGHTvJ_SF"},"source":["fig2 = m.plot_components(forecast)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8D1P6pjLJ_SG"},"source":["Things are a lot smoother now. Note that in the original kernel, I believe that \"NFL Sundays\" wasn't included in the future data- so the results look slightly different. "]},{"cell_type":"code","metadata":{"id":"PHV59tijJ_SG"},"source":["ps1i1 = forecast[[\"ds\"]]\n","ps1i1[\"forecast\"] = np.expm1(forecast[\"yhat\"])\n","ps1i1[\"yearmonth\"] = pd.to_datetime(ps1i1[\"ds\"]).dt.to_period(\"M\")\n","ps1i1.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CDoh_2upJ_SG"},"source":["#### 1.3.8.7 Accuracy calculation"]},{"cell_type":"code","metadata":{"id":"pndXYsYmJ_SG"},"source":["def smape(outsample, forecast):\n","    num = np.abs(outsample-forecast)\n","    denom = np.abs(outsample) + np.abs(forecast)\n","    return (num/denom)/2\n","\n","stats[\"ds\"] = pd.to_datetime(stats[\"ds\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCmeeWKsJ_SG"},"source":["ps1i1[\"ds\"] = pd.to_datetime(ps1i1[\"ds\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h6jR6X_J_SH"},"source":["train_predict = stats.merge(ps1i1)\n","smape_err = smape(train_predict[\"y\"], train_predict[\"forecast\"])\n","smape_err = smape_err[~np.isnan(smape_err)]\n","np.mean(smape_err)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KP09mgTmJ_SH"},"source":["### 1.3.9 Automated forecasting with Prophet: Splitting data by store and item"]},{"cell_type":"code","metadata":{"id":"nzi4Z02bJ_SI"},"source":["##Training data from the very beginning\n","# Note that I've added some columns\n","df_train[\"sales\"] = np.log1p(df_train[\"sales\"]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izeVyUwkJ_SI"},"source":["df_train[\"year\"] = pd.to_datetime(df_train[\"date\"]).dt.year\n","df_train[\"month\"] = pd.to_datetime(df_train[\"date\"]).dt.month\n","df_train[\"month_year\"] = pd.to_datetime(df_train[\"date\"]).dt.to_period('M')\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWEUj6ODJ_SI"},"source":["df_train.columns = [\"ds\", \"store\", \"item\", \"sales\", \"y\", \"m\", \"my\"]\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBInUcH7J_SI"},"source":["def make_prediction(df):\n","    \n","    playoffs = ['2013-07-12', '2014-07-12', '2014-07-19',\n","                 '2014-07-02', '2015-07-11', '2016-07-17',\n","                 '2016-07-24', '2016-07-07','2016-07-24']\n","    superbowl = ['2013-01-01', '2013-12-25', '2014-01-01', '2014-12-25','2015-01-01', '2015-12-25','2016-01-01', '2016-12-25',\n","                    '2017-01-01', '2017-12-25']\n","\n","    playoffs = pd.DataFrame({\n","      'holiday': 'playoff',\n","      'ds': pd.to_datetime(playoffs),\n","      'lower_window': 0,\n","      'upper_window': 1,\n","    })\n","    superbowls = pd.DataFrame({\n","      'holiday': 'superbowl',\n","      'ds': pd.to_datetime(superbowl),\n","      'lower_window': 0,\n","      'upper_window': 1,\n","    })\n","    holidays = pd.concat((playoffs, superbowls))\n","    \n","    m = Prophet(holidays=holidays, holidays_prior_scale=0.5,\n","            yearly_seasonality=4,  interval_width=0.95,\n","            changepoint_prior_scale=0.006, daily_seasonality=True)\n","    m.add_seasonality(name='daily', period=60, fourier_order=5)\n","    m.fit(df)\n","    future = m.make_future_dataframe(periods=90)\n","    forecast = m.predict(future)\n","    return forecast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUB-qSFjJ_SJ"},"source":["df = df_train[(df_train[\"store\"]==1) & (df_train[\"item\"] ==2)]\n","df = df[[\"ds\", \"sales\"]]\n","df.columns = [\"ds\", \"y\"]\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NnkbjLCeJ_SJ"},"source":["prediction = make_prediction(df)\n","prediction[[\"ds\", \"yhat\"]].tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aFos2vylJ_SJ"},"source":["## 1.3.10 TIME SERIES ANALYSIS - ARIMA -\n","1. [Stationarity Check](#StationarityCheck)\n","2. [Data Transformation to achieve Stationarity](#DataTransformation)\n","3. [Plotting ACF & PACF](#PlottingACF-PACF)\n","4. [Train Test split and Building Models](#BuildingARIMAModel)\n","5. [Test the model](#Test)\n","6. [Conclusion](#Conclusion)"]},{"cell_type":"markdown","metadata":{"id":"v0IfmDT0J_SK"},"source":["#### 1.3.10.1 Stationarity Check <a name = \"StationarityCheck\"></a>\n"]},{"cell_type":"code","metadata":{"id":"vlSo7KSAJ_SK"},"source":["from statsmodels.tsa.stattools import adfuller\n","from statsmodels.tsa.arima_model import ARIMA\n","from sklearn.metrics import mean_squared_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcE2oTylJ_SK"},"source":["def evaluate_arima_model(X, arima_order):\n","    # prepare training dataset\n","    train_size = int(len(X) * 0.66)\n","    train, test = X[0:train_size], X[train_size:]\n","    history = [x for x in train]\n","    # make predictions\n","    predictions = list()\n","    for t in range(len(test)):\n","        model = ARIMA(history, order=arima_order)\n","        model_fit = model.fit(disp=0)\n","        yhat = model_fit.forecast()[0]\n","        predictions.append(yhat)\n","        history.append(test[t])\n","    # calculate out of sample error\n","    error = mean_squared_error(test, predictions)\n","    return error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtO4xgeZJ_SK"},"source":["# evaluate combinations of p, d and q values for an ARIMA model\n","def evaluate_models(dataset, p_values, d_values, q_values):\n","    dataset = dataset.astype('float32')\n","    best_score, best_cfg = float(\"inf\"), None\n","    for p in p_values:\n","        for d in d_values:\n","            for q in q_values:\n","                order = (p,d,q)\n","                try:\n","                    mse = evaluate_arima_model(dataset, order)\n","                    if mse < best_score:\n","                        best_score, best_cfg = mse, order\n","                    print('ARIMA%s MSE=%.3f' % (order,mse))\n","                except:\n","                    continue\n","    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7WHIG-UJ_SL"},"source":["df_train.set_index('date',inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW4mUDHVJ_SL"},"source":["m = max(df_train[\"store\"])\n","n = max(df_train[\"item\"])\n","k = df_train.shape[0]\n","p = int(k/(m*n))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAYh5P_XJ_SL"},"source":["#slice by different store and item\n","ts_train = df_train.iloc[0:p, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WarKHveWJ_SL"},"source":["#Determing rolling statistics\n","rolmean = pd.Series.rolling(ts_train, window=12).mean()\n","rolstd = pd.Series.rolling(ts_train, window=12).std()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nxl2UvqEJ_SL"},"source":["#Plot rolling statistics:\n","orig = plt.plot(ts_train, color=\"blue\",label=\"Original\")\n","mean = plt.plot(rolmean, color=\"red\", label=\"Rolling Mean\")\n","std = plt.plot(rolstd, color=\"black\", label = \"Rolling Std\")\n","plt.legend(loc=\"best\")\n","plt.title(\"Rolling Mean & Standard Deviation\")\n","plt.show(block=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yZm4CMaJ_SM"},"source":["#Perform Dickey-Fuller test:\n","print (\"Results of Dickey-Fuller Test:\")\n","dftest = adfuller(ts_train, autolag=\"AIC\")\n","dfoutput = pd.Series(dftest[0:4], index=[\"Test Statistic\",\"p-value\",\"#Lags Used\",\"Number of Observations Used\"])\n","for key,value in dftest[4].items():\n","    dfoutput[\"Critical Value (%s)\"%key] = value\n","print (dfoutput)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZV5zvR6J_SM"},"source":["The Dickey-Fuller test the time series is not considered stationary as Test Statistic greater than 1% Critical Value and we can see visually that there is an upwards trend.\n","\n"]},{"cell_type":"code","metadata":{"id":"gO1KjT3ZJ_SM"},"source":["warnings.filterwarnings(\"ignore\")\n","evaluate_models(ts_train, [0,1,2], [0,1,2], [0,1,2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_sHhS_SJ_SM"},"source":["#### 1.3.10.2 Data Transformation to achieve Stationarity <a name = \"DataTransformation\"></a>"]},{"cell_type":"markdown","metadata":{"id":"xENfkujJJ_SM"},"source":["Try to make the data __Stationary__ we will try"]},{"cell_type":"markdown","metadata":{"id":"Gc3dv8jFJ_SN"},"source":["1. Log scale transform\n","2. Log Scale Minus Moving Average\n","3. Exponential Decay\n","4. Shift Difference"]},{"cell_type":"code","metadata":{"id":"VJOtXmMqJ_SN"},"source":["df_train.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VO4QQHtgJ_SN"},"source":["##### 1.3.10.2.1 log scale transform"]},{"cell_type":"code","metadata":{"id":"kYIxKycTJ_SN"},"source":["#log scale transform\n","import numpy as np\n","df_train['logsales']  =  np.log(df_train['sales'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VI-YBCuBJ_SN"},"source":["# Determine rolling statistics\n","def rolling_means(time_series):\n","    rolmean  =  time_series.rolling(window = 12).mean()    #window size 12 denotes 12 months, giving rolling mean at yearly level\n","    rolstd  =  time_series.rolling(window = 12).std()\n","\n","    orig  =  plt.plot(time_series, color = 'blue', label = 'Original')\n","    mean  =  plt.plot(rolmean, color = 'red', label = 'Rolling Mean')\n","    std  =  plt.plot(rolstd, color = 'black', label = 'Rolling Std')\n","    plt.legend(loc = 'best')\n","    plt.title('Rolling Mean & Standard Deviation')\n","    plt.show(block = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lSgt0D6J_SO"},"source":["#Perform Augmented Dickey–Fuller test:\n","def adf_test(time_series):\n","    \n","    from statsmodels.tsa.stattools import adfuller\n","    dftest  =  adfuller(time_series, autolag = 'AIC')\n","\n","    dfoutput  =  pd.Series(dftest[0:4], index = ['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n","    for key,value in dftest[4].items():\n","        \n","        dfoutput['Critical Value (%s)'%key]  =  value\n","    \n","    print('Results of Dickey Fuller Test:') \n","    print(dfoutput)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxZtFfr4J_SO"},"source":["indexedDataset = df_train.loc[(df_train['store'] == 1) & (df_train['item'] == 1)]\n","indexedDataset= indexedDataset.drop(columns=[\"store\",\"item\"])\n","indexedDataset.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VaFNu9AJ_SP"},"source":["adf_test(indexedDataset.logsales)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hx1zlfmAJ_SP"},"source":["rolling_means(indexedDataset.drop(columns=['sales']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3Sn8zz9J_SP"},"source":["Dataset is still non stationary\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VaNGHnjnJ_SP"},"source":["##### 1.3.10.2.2 Log Scale Minus Moving Average\n"]},{"cell_type":"code","metadata":{"id":"5UNA26DNJ_SQ"},"source":["indexedDataset.drop(columns=['sales'],inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxSk4BnBJ_SQ"},"source":["movingAverage  =  indexedDataset.rolling(window = 12).mean()\n","datasetLogScaleMinusMovingAverage  =  indexedDataset - movingAverage\n","\n","#Remove NAN values\n","datasetLogScaleMinusMovingAverage.dropna(inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGn1XrgjJ_SQ"},"source":["rolling_means(datasetLogScaleMinusMovingAverage)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7hJjOg4J_SQ"},"source":["adf_test(datasetLogScaleMinusMovingAverage.logsales)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvtq72SIJ_SQ"},"source":["Data is somewhat stationary now\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GVqg4eNMJ_SR"},"source":["##### 1.3.10.2.3 Exponential Decay Weighted Average"]},{"cell_type":"code","metadata":{"id":"vUcRnt3HJ_SR"},"source":["exponentialDecayWeightedAverage  =  indexedDataset.ewm(halflife = 12, min_periods = 0, adjust = True).mean()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEsqhbxZJ_SR"},"source":["rolling_means(exponentialDecayWeightedAverage)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oImmUh5QJ_SR"},"source":["adf_test(exponentialDecayWeightedAverage.logsales)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmh0pUnWJ_SS"},"source":["##### 1.3.10.2.4 Log Scale Shifting"]},{"cell_type":"code","metadata":{"id":"DEwuiLroJ_SS"},"source":["datasetLogDiffShifting  =  indexedDataset - indexedDataset.shift()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_BaR5jCJ_SS"},"source":["datasetLogDiffShifting.dropna(inplace = True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUpBqLtKJ_SS"},"source":["datasetLogDiffShifting.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tYk1QaHJ_SS"},"source":["adf_test(datasetLogDiffShifting.logsales)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSPptGP9J_ST"},"source":["rolling_means(datasetLogDiffShifting)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOIkshqrJ_ST"},"source":["the p-value is extremely small. Thus this series is very likely to be stationary.\n","Data is now Stationary\n"]},{"cell_type":"code","metadata":{"id":"mSi7jDdCJ_ST"},"source":["plt.plot(datasetLogDiffShifting)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jb4PP_pmJ_ST"},"source":["#### 1.3.10.3 Plotting ACF & PACF <a name = \"PlottingACFPACF\"></a>"]},{"cell_type":"code","metadata":{"id":"CFo0DK_xJ_ST"},"source":["from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","\n","fig = plt.figure(figsize=(12,8))\n","ax1 = fig.add_subplot(211)\n","fig = plot_acf(datasetLogDiffShifting.logsales, lags=80, ax=ax1) \n","ax2 = fig.add_subplot(212)\n","fig = plot_pacf(datasetLogDiffShifting.logsales, lags=80, ax=ax2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ZUSTOfGJ_SU"},"source":["P, Q value - 7\n","\n","Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists."]},{"cell_type":"markdown","metadata":{"id":"7G2alwIzJ_SU"},"source":["#### 1.3.10.4 Train Test split and Building ARIMA Model <a name = \"BuildingARIMAModel\"></a> "]},{"cell_type":"code","metadata":{"id":"qfSwkujJJ_SU"},"source":["datasetLogDiffShifting.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hMdSX9GJ_SU"},"source":["# Data Preparation\n","#testing last 100 days\n","train, test = datasetLogDiffShifting[0:len(datasetLogDiffShifting)-100], datasetLogDiffShifting[len(datasetLogDiffShifting)-100:]\n","train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQ4Y67toJ_SU"},"source":["test.plot()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NNzcDxPJ_SV"},"source":["\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GUYSJwzJ_SV"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","model = ARIMA (train, order=(7,0,7))\n","model_fit = model.fit()\n","print(model_fit.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ysh0wdgvJ_SV"},"source":["# plot residual errors\n","residuals = pd.DataFrame(model_fit.resid)\n","residuals.plot()\n","plt.show()\n","residuals.plot(kind='kde')\n","plt.show()\n","print(residuals.describe())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtvUHhreJ_SV"},"source":["model_fit.fittedvalues.head()\n","print(train.shape)\n","print(model_fit.fittedvalues.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YL_ibcR2J_SV"},"source":["from sklearn.metrics import mean_squared_error\n","from matplotlib import pyplot\n","error = mean_squared_error(train, model_fit.fittedvalues)\n","print('Train MSE: %.3f' % error)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OS88XZhdJ_SW"},"source":["train.head(10).plot()\n","model_fit.fittedvalues.head(10).plot(color='red')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGnJWyZEJ_SW"},"source":["#### 1.3.10.5 Test Model <a name = \"Test\"></a>\n"]},{"cell_type":"code","metadata":{"id":"3bDyFMfAJ_SW"},"source":["# make predictions\n","predictions = model_fit.predict(start=len(train), end=len(datasetLogDiffShifting)-1, dynamic=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrWCTmuyJ_SW"},"source":["error = mean_squared_error(test, predictions)\n","print('Test MSE: %.3f' % error)\n","# plot results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHT-8cbGJ_SX"},"source":["pyplot.plot(test)\n","pyplot.plot(predictions, color='red')\n","pyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugjZf52PJ_SX"},"source":["#### 1.3.10.6 Conclusion <a name = \"Conclusion\"></a>"]},{"cell_type":"markdown","metadata":{"id":"oVdgzSjKJ_SX"},"source":["We have built a model to forecast the demand of item 1 sales in a Store 1 </br>\n","Using ARIMA model with P value and Q value 7 Since we observed the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists."]},{"cell_type":"markdown","metadata":{"id":"xKNujGAUJ_SX"},"source":["### 1.3.11 How to run the Custom Notebook on Azure Compute Clusters and Deployment on Azure AKS Clusters\n","\n","The above notebook code can also be run inside Azure Compute clusters and the Trained Model can be deployed on Azure AKS Cluster as a Web Service.\n","\n","Kindly refer to below innersource code repo for getting detailed steps to do so\n","\n","https://innersource.soprasteria.com/aeroline-ai/data-science/azure-ml-compute-cluster-aks-example.\n","\n","Kindly reach out to Admin in case of any access issues to the above code repo"]},{"cell_type":"markdown","metadata":{"id":"ZkTeCvYXJ_SX"},"source":["## 1.4 Comparision of Microsoft Products & Custom Code Results on Pump Sensors Dataset (Azure Data Explorer & Custom Code)\n","\n","Lets compare the results for any particular Sensor (ex:- Sensor 12)"]},{"cell_type":"markdown","metadata":{"id":"-sbwT2O2J_SY"},"source":["### 1.4.1 Azure Data Explorer Result\n","PFA the Azure Data Explorer Result for Anomaly Detection (Sensor12)\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"GtbR5k-lJ_SY"},"source":["### 1.4.2 Custom Notebook Code Result\n","\n","PFA the Custom code result for Anomaly Detection (Sensor12)\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"fLaFB67eJ_SY"},"source":["### 1.4.3 Conclusion\n","\n","It seems there is slight difference between results from Custom Code and Azure Data Explorer"]},{"cell_type":"markdown","metadata":{"id":"dJGqQJxyJ_SY"},"source":["# 1.5 Some links on Feature Engineering\n","\n","Here are some of link which showcases some information on Feature Engineering.\n","\n","https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114#7559"]},{"cell_type":"markdown","metadata":{"id":"bvWgZWMYJ_SY"},"source":["# 1.6 Some links on HyperParameter Tuning\n","\n","Here are some of the links on Hyperparameter Tunning.\n","\n","https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568"]},{"cell_type":"markdown","metadata":{"id":"HlibTKF4J_SZ"},"source":["# 1.7 Production Deployment \n","\n","Kindly refer to 'Anomaly_Detection_Solutons_Production_Deployment_Azure.ipynb' file for guidance on Production Deployment\n","https://innersource.soprasteria.com/aeroline-ai/cookbooks/cookbook-time-series-anomaly-detection/-/blob/master/azure-cookbook/Anomaly_Detection_Solutons_Production_Deployment_Azure.ipynb"]},{"cell_type":"code","metadata":{"id":"_vzj4WL-J_SZ"},"source":[""],"execution_count":null,"outputs":[]}]}