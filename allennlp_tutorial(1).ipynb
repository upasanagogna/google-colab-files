{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"allennlp_tutorial.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2uXCFx3q_SL8"},"source":["## **AllenNLP demo**\n","\n","This is a demo for prediction venue based on title and abstract of the paper\n","\n","reference: https://github.com/allenai/allennlp-as-a-library-example"]},{"cell_type":"code","metadata":{"id":"AN-4hf6D_bL6","executionInfo":{"status":"ok","timestamp":1604858075189,"user_tz":-330,"elapsed":63540,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}},"outputId":"2163c011-2bf7-4a7b-d9a0-df939f5a652b","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install allennlp"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/14/f0f9dd1ce012e7723742821b95b33dd9bdc53befe209600608bc7be1f650/allennlp-1.2.0-py3-none-any.whl (498kB)\n","\u001b[K     |████████████████████████████████| 501kB 3.3MB/s \n","\u001b[?25hCollecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n","Collecting overrides==3.1.0\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n","Requirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.7.0+cu101)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n","Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.5)\n","Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 11.5MB/s \n","\u001b[?25hCollecting boto3<2.0,>=1.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/54/099a2ea5d4b2d5931a26f280a7585f613b1fafaac9189e489a9e25004a01/boto3-1.16.13-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 17.5MB/s \n","\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n","\u001b[K     |████████████████████████████████| 266kB 17.9MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.7)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n","Collecting transformers<3.5,>=3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 16.8MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (2.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (0.16.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (50.3.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.4)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.15.0)\n","Collecting botocore<1.20.0,>=1.19.13\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/40/b5e681d80dc46bafd0dc2e55266190cc432dfd5b72b9e7e1c5743aa6c362/botocore-1.19.13-py2.py3-none-any.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 28.5MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.17.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.2.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.9.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.6.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Collecting tokenizers==0.9.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 39.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 44.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.5,>=3.1->allennlp) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.5,>=3.1->allennlp) (20.4)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 44.5MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.4.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.13->boto3<2.0,>=1.14->allennlp) (2.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.5,>=3.1->allennlp) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.5,>=3.1->allennlp) (2.4.7)\n","Building wheels for collected packages: overrides, jsonnet, sacremoses\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=14c48ee1fd8f89180cedafbabd4fafb6b79009a960810985100cd65d801dec02\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321594 sha256=5f6b89634bab28d4caee9d76d7e9bb707bdddacf4ab67422b88ff3c899d20d6c\n","  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c382a55f909822516918067348c72724da24193f59ae6bf1a9e58568ad8c8839\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built overrides jsonnet sacremoses\n","\u001b[31mERROR: botocore 1.19.13 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jsonpickle, overrides, tensorboardX, jmespath, botocore, s3transfer, boto3, jsonnet, tokenizers, sacremoses, sentencepiece, transformers, allennlp\n","Successfully installed allennlp-1.2.0 boto3-1.16.13 botocore-1.19.13 jmespath-0.10.0 jsonnet-0.16.0 jsonpickle-1.4.1 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tensorboardX-2.1 tokenizers-0.9.2 transformers-3.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6p7i_4dH_SMA","executionInfo":{"status":"error","timestamp":1604857989462,"user_tz":-330,"elapsed":3770,"user":{"displayName":"Upasana Gogna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF152qiue1VxC4QwqNWg_qUQ2Dsf6UQhenBLsa6g=s64","userId":"17402908673397364006"}},"outputId":"66971f2f-315d-44d1-d307-e316ba5146b8","colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["import json\n","from typing import Iterator, List, Dict, Optional\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# for dataset reader\n","from allennlp.data import Instance\n","from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n","from allennlp.data.dataset_readers import DatasetReader\n","from allennlp.common.file_utils import cached_path\n","from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n","from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n","from allennlp.data.vocabulary import Vocabulary\n","\n","# read pretrained embedding from AWS S3\n","from allennlp.modules.token_embedders.embedding import _read_embeddings_from_text_file\n","\n","# for building model\n","from allennlp.models import Model\n","from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n","from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n","from allennlp.modules import FeedForward\n","from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n","from allennlp.nn import InitializerApplicator, RegularizerApplicator\n","from allennlp.training.metrics import CategoricalAccuracy\n","from allennlp.data.iterators import BucketIterator\n","from allennlp.training.trainer import Trainer"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6b2ae605186f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# for dataset reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"l1K7V-Rq_SMF"},"source":["## **Create classes for the model**\n","\n","Generally, we need to implement 2 classes for AllenNLP including\n","\n","- `DatasetReader`: to read dataset and return `Instance` class\n","- `Model`: input `Instance` class and return output prediction\n","\n","`Model` consists of the Sequence to Vector model (`Seq2Vec`)\n","\n","<img src=\"figures/bilstm.png\" width=\"300\"/>\n","\n","\n","and we use the combination of vectors to predict venue\n","\n","<img src=\"figures/venue_prediction.png\" width=\"300\"/>"]},{"cell_type":"code","metadata":{"id":"JKo0Iy9X_SMG"},"source":["class PublicationDatasetReader(DatasetReader):\n","    \"\"\"\n","    DatasetReader for publication and venue dataaet\n","    \"\"\"\n","    def __init__(self, \n","                 tokenizer: Tokenizer = None,\n","                 token_indexers: Dict[str, TokenIndexer] = None, \n","                 lazy: bool = False) -> None:\n","        super().__init__(lazy)\n","        self._tokenizer = tokenizer or WordTokenizer()\n","        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n","\n","    def _read(self, file_path: str) -> Iterator[Instance]:\n","        \"\"\"\n","        Read publication and venue dataset in JSON format\n","        \n","        Data is in the following format:\n","            {\"title\": ..., \"paperAbstract\": ..., \"venue\": ...}\n","        \"\"\"\n","        with open(cached_path(file_path), \"r\") as data_file:\n","            for line in data_file:\n","                line = line.strip(\"\\n\")\n","                if not line:\n","                    continue\n","                paper_json = json.loads(line)\n","                title = paper_json['title']\n","                abstract = paper_json['paperAbstract']\n","                venue = paper_json['venue']\n","                yield self.text_to_instance(title, abstract, venue)\n","        \n","    def text_to_instance(self, \n","                         title: str, \n","                         abstract: str, \n","                         venue: str=None) -> Instance:\n","        \"\"\"\n","        Turn title, abstract, and venue to instance\n","        \"\"\"\n","        tokenized_title = self._tokenizer.tokenize(title)\n","        tokenized_abstract = self._tokenizer.tokenize(abstract)\n","        title_field = TextField(tokenized_title, self._token_indexers)\n","        abstract_field = TextField(tokenized_abstract, self._token_indexers)\n","        fields = {'title': title_field, \n","                  'abstract': abstract_field}\n","        if venue is not None:\n","            fields['label'] = LabelField(venue)\n","        return Instance(fields)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DW0a_Shs_SMJ"},"source":["class AcademicPaperClassifier(Model):\n","    \"\"\"\n","    Model to classify venue based on input title and abstract\n","    \"\"\"\n","    def __init__(self, \n","                 vocab: Vocabulary,\n","                 text_field_embedder: TextFieldEmbedder,\n","                 title_encoder: Seq2VecEncoder,\n","                 abstract_encoder: Seq2VecEncoder,\n","                 classifier_feedforward: FeedForward,\n","                 initializer: InitializerApplicator = InitializerApplicator(),\n","                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n","        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)\n","        self.text_field_embedder = text_field_embedder\n","        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n","        self.title_encoder = title_encoder\n","        self.abstract_encoder = abstract_encoder\n","        self.classifier_feedforward = classifier_feedforward\n","        self.metrics = {\n","                \"accuracy\": CategoricalAccuracy(),\n","                \"accuracy3\": CategoricalAccuracy(top_k=3)\n","        }\n","        self.loss = torch.nn.CrossEntropyLoss()\n","        initializer(self)\n","    \n","    def forward(self, \n","                title: Dict[str, torch.LongTensor],\n","                abstract: Dict[str, torch.LongTensor],\n","                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n","        \n","        embedded_title = self.text_field_embedder(title)\n","        title_mask = get_text_field_mask(title)\n","        encoded_title = self.title_encoder(embedded_title, title_mask)\n","\n","        embedded_abstract = self.text_field_embedder(abstract)\n","        abstract_mask = get_text_field_mask(abstract)\n","        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)\n","\n","        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))\n","        class_probabilities = F.softmax(logits, dim=-1)\n","        argmax_indices = np.argmax(class_probabilities.cpu().data.numpy(), axis=-1)\n","        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\") for x in argmax_indices]\n","        output_dict = {\n","            'logits': logits, \n","            'class_probabilities': class_probabilities,\n","            'predicted_label': labels\n","        }\n","        if label is not None:\n","            loss = self.loss(logits, label)\n","            for metric in self.metrics.values():\n","                metric(logits, label)\n","            output_dict[\"loss\"] = loss\n","\n","        return output_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3bP74_B_SMN"},"source":["## **Read dataset**\n","\n","- `cached_path`: can cache the file locally\n","- `BasicTextFieldEmbedder` takes a mapping from index names to embeddings"]},{"cell_type":"code","metadata":{"id":"sl7__nw2_SMN"},"source":["train_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/train.jsonl\"\n","validation_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/dev.jsonl\"\n","pretrained_file = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tFPUmfa_SMR"},"source":["reader = PublicationDatasetReader()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-TgFu7W_SMU"},"source":["instance = reader.text_to_instance(\"This is a great paper.\", \n","                                   \"Indeed, this is a great paper of all time\", \n","                                   \"Nature\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ylrV1x6O_SMX","outputId":"d5e7150a-19cb-420e-862b-9c3552652e8d"},"source":["train_dataset = reader.read(cached_path(train_data_path))\n","validation_dataset = reader.read(cached_path(validation_data_path))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15000it [01:12, 206.74it/s]\n","2000it [00:10, 199.56it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Z2Y47LQ5_SMd","outputId":"ea6f53ed-6359-4563-cc94-e1cb95583ecb"},"source":["# building vocabulary\n","vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 17000/17000 [00:05<00:00, 3205.53it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1yu-FhRd_SMi","outputId":"ce66836e-5f4a-4e0c-ba11-f66a7ea56019"},"source":["# load pre-trained embedding\n","embedding_matrix = _read_embeddings_from_text_file(file_uri=pretrained_file, \n","                                                   embedding_dim=100, \n","                                                   vocab=vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["400000it [00:03, 100679.01it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qyjg6ylK_SMl","outputId":"7ae7430f-e18b-45b8-9a26-62cada87ad3c"},"source":["print(embedding_matrix.size()) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([64714, 100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lN0WNjvJ_SMo"},"source":["EMBEDDING_DIM = 100\n","HIDDEN_DIM = 100\n","num_classes = len(vocab.get_index_to_token_vocabulary('labels'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOlww4q1_SMt"},"source":["# embedding\n","token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n","                            embedding_dim=EMBEDDING_DIM,\n","                            weight=embedding_matrix)\n","word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuIxm5_8_SMw"},"source":["lstm_title = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n","                                                 batch_first=True, bidirectional=True))\n","lstm_abstract = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n","                                                    batch_first=True, bidirectional=True))\n","feed_forward = torch.nn.Linear(2 * 2 * HIDDEN_DIM, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DkrOaKK_SMz"},"source":["model = AcademicPaperClassifier(vocab,\n","                                word_embeddings, \n","                                lstm_title, \n","                                lstm_abstract, \n","                                feed_forward)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlamq_OO_SM3"},"source":["optimizer = optim.SGD(model.parameters(), lr=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHmLBt_Z_SM7"},"source":["iterator = BucketIterator(batch_size=64, \n","                          sorting_keys=[(\"abstract\", \"num_tokens\"), \n","                                        (\"title\", \"num_tokens\")])\n","iterator.index_with(vocab) # index with the created vocabulary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZuN2du7_SM9"},"source":["trainer = Trainer(\n","    model=model,\n","    optimizer=optimizer,\n","    iterator=iterator,\n","    train_dataset=train_dataset,\n","    validation_dataset=validation_dataset,\n","    patience=2,\n","    num_epochs=5,\n","    serialization_dir='output'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"g4Kcv0z9_SNB","outputId":"62096397-367e-4395-9679-b659148a34eb"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loss: 0.7956 ||: 100%|██████████| 235/235 [06:06<00:00,  1.56s/it]\n","loss: 0.6613 ||: 100%|██████████| 32/32 [00:05<00:00,  5.60it/s]\n","loss: 0.5951 ||: 100%|██████████| 235/235 [06:54<00:00,  1.77s/it]\n","loss: 0.6056 ||: 100%|██████████| 32/32 [00:10<00:00,  3.14it/s]\n","loss: 0.5466 ||: 100%|██████████| 235/235 [07:42<00:00,  1.97s/it]\n","loss: 0.5737 ||: 100%|██████████| 32/32 [00:07<00:00,  4.56it/s]\n","loss: 0.5181 ||: 100%|██████████| 235/235 [07:27<00:00,  1.91s/it]\n","loss: 0.5616 ||: 100%|██████████| 32/32 [00:07<00:00,  4.00it/s]\n","loss: 0.5016 ||: 100%|██████████| 235/235 [07:58<00:00,  2.04s/it]\n","loss: 0.5476 ||: 100%|██████████| 32/32 [00:08<00:00,  3.94it/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'training_duration': '00:36:48',\n"," 'training_start_epoch': 0,\n"," 'training_epochs': 4,\n"," 'epoch': 4,\n"," 'training_loss': 0.5016463198560349,\n"," 'validation_loss': 0.5475732861086726,\n"," 'best_epoch': 4,\n"," 'best_validation_loss': 0.5475732861086726}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"Yr763ets_SNE"},"source":["## **Prediction**\n","\n","Lastly, we can also write the prediction class `PaperClassifierPredictor` which take input any `json_dict` and return the `Instance`. The AllenNLP will take care of the prediction."]},{"cell_type":"code","metadata":{"id":"os4PdNbo_SNF"},"source":["from allennlp.common.util import JsonDict\n","from allennlp.predictors.predictor import Predictor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zz7j52xz_SNH"},"source":["class PaperClassifierPredictor(Predictor):\n","    \"\"\"\"\n","    Predictor wrapper for the AcademicPaperClassifier\n","    \"\"\"\n","    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n","        title = json_dict['title']\n","        abstract = json_dict['paperAbstract']\n","        instance = self._dataset_reader.text_to_instance(title=title, abstract=abstract)\n","        return instance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YP-xwdpF_SNN"},"source":["predictor = PaperClassifierPredictor(model, dataset_reader=reader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Dqb5SDj_SNP","outputId":"57dca909-6e62-4b5d-a7dc-dd5a3eae04a3"},"source":["prediction_output = predictor.predict_json(\n","    {\n","        \"title\": \"Know What You Don't Know: Unanswerable Questions for SQuAD\", \n","        \"paperAbstract\": \"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.\"\n","    }\n",")\n","\n","print(prediction_output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'logits': [1.7778292894363403, 0.11821205168962479, -1.8632274866104126], 'class_probabilities': [0.8220734000205994, 0.15636803209781647, 0.02155855856835842], 'predicted_label': 'ACL'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M5-A4fVG_SNS"},"source":["## **Load model for prediction**\n","\n","Here, we trained the model and save it in `output` folder using \n","\n","```\n","allennlp train example_training.json -s output --include-package venue\n","```\n","\n","`venue` is a library that we created where we make AllenNLP as a library. We can load trained model (`model.tar.gz`) from `serialization_dir` (`output`) and use it to predict the classes."]},{"cell_type":"code","metadata":{"id":"3NRVa5E5_SNT","outputId":"8deb1000-2372-4c98-e173-c4764e31903f"},"source":["from allennlp.models.archival import load_archive\n","from allennlp.predictors.predictor import Predictor\n","from venue.venue_predictor import PaperClassifierPredictor\n","from venue.venue_reader import PublicationDatasetReader\n","from venue.venue_classifier import AcademicPaperClassifier\n","\n","archive = load_archive('output/model.tar.gz')\n","venue_predictor = Predictor.from_archive(archive, 'venue_predictor')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/Users/titipata/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tk6OuE-W_SNW","outputId":"e8b54397-6f95-4332-9446-886a80c61e23"},"source":["prediction_output = venue_predictor.predict_json(\n","    {\n","        \"title\": \"Know What You Don't Know: Unanswerable Questions for SQuAD\", \n","        \"paperAbstract\": \"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.\"\n","    }\n",")\n","\n","print(prediction_output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'logits': [1.3005162477493286, 0.6157346367835999, -2.2390244007110596], 'class_probabilities': [0.6522191762924194, 0.3288491368293762, 0.018931740894913673], 'predicted_label': 'ACL'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j0nhmhv4_SNa","outputId":"b7d2ebc9-1ce6-47da-f4a1-c7041906c346"},"source":["venue_predictor._model.vocab.get_index_to_token_vocabulary('labels') # all classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'ACL', 1: 'AI', 2: 'ML'}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"1Npm65i-_SNd"},"source":["## **Predict which journals to submit from Medline**\n","\n","We  do a fun experiment where we train the same model to classify \n","publications from sample 110 journals from MEDLINE. We got accuracy of 64.9 percent on the validation dataset."]},{"cell_type":"code","metadata":{"id":"X5R2caz1_SNe"},"source":["import torch\n","from allennlp.models.archival import load_archive\n","from allennlp.predictors.predictor import Predictor\n","from venue.venue_predictor import PaperClassifierPredictor\n","from venue.venue_reader import PublicationDatasetReader\n","from venue.venue_classifier import AcademicPaperClassifier\n","from allennlp.common.file_utils import cached_path\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mg38j4V_SNj","outputId":"d1a68687-4772-4556-aa97-f32e3f915fcc"},"source":["archive = load_archive(cached_path('https://s3-us-west-2.amazonaws.com/allennlp-tutorial/model.tar.gz'))\n","venue_predictor = Predictor.from_archive(archive, 'venue_predictor')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/Users/titipata/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"G3dwJRug_SNm"},"source":["title = \"\"\"\n","Modeling peripheral visual acuity enables discovery of gaze strategies \n","at multiple time scales during natural scene search\n","\"\"\"\n","abstract = \"\"\"\n","Like humans, monkeys make saccades nearly three times a second. \n","To understand the factors guiding this frequent decision, computational models of vision \n","attempt to predict fixation locations using bottom-up visual features and top-down goals. \n","How do the relative influences of these factors evolve over multiple time scales? \n","Here we analyzed visual features at fixations using a retinal transform that provides realistic \n","visual acuity by suitably degrading visual information in the periphery. \n","In a task in which monkeys searched for a Gabor target in natural scenes, we characterized \n","the relative importance of bottom-up and task-relevant influences by decoding fixated from \n","nonfixated image patches based on visual features. At fast time scales, we found that search \n","strategies can vary over the course of a single trial, with locations of higher saliency, target-similarity, \n","edge–energy, and orientedness looked at later on in the trial. At slow time scales, we found that \n","search strategies can be refined over several weeks of practice, and the influence of target orientation \n","was significant only in the latter of two search tasks. Critically, these results were not observed without \n","applying the retinal transform. Our results suggest that saccade-guidance strategies become apparent only \n","when models take into account degraded visual representation in the periphery.'\n","\"\"\"\n","prediction_output = venue_predictor.predict_json(\n","    {\n","        \"title\": title, \n","        \"paperAbstract\": abstract\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpVhR6Mp_SNp"},"source":["venues = venue_predictor._model.vocab.get_index_to_token_vocabulary('labels') # all classes\n","venues = [venues[i] for i in range(len(venues))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19vHq1ZU_SNs","outputId":"3d3bb97b-f28f-4053-a082-887dc3b586b1"},"source":["# rank top 5 which journal to submit\n","sorted(list(zip(venues, prediction_output['class_probabilities'])), \n","       key=lambda x: x[1], reverse=True)[0:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Journal of vision', 0.4988258183002472),\n"," ('Frontiers in psychology', 0.40092626214027405),\n"," ('Journal of neurophysiology', 0.048388607800006866),\n"," ('The Journal of neuroscience : the official journal of the Society for Neuroscience',\n","  0.03905167803168297),\n"," ('PLoS computational biology', 0.006291377358138561)]"]},"metadata":{"tags":[]},"execution_count":5}]}]}